Review of generalized linear point-process models
Neurostats club 2015



#############################################################################
$Introduction
$$

@
{center}
    \large This will be quick\\
    No formal derivation\\
    No proofs


@Truccolo et al. 2005
    Derivation of point process GLM
    \emph{Intrinsic and ensemble history models}
    \emph{Maximum likelihood estimation}
    Non-GLM conditional intensity model
    KS test and time rescaling
    Residual analysis
    Model selection
    Decoding
    \emph{Take NEUR2110 with Wilson Truccolo in the spring!}
    If there is time: Closed form approximation, double crossvalidation, regularization, and regularization paths

#############################################################################
    
@Linear model / multiple linear regression
\[y = \beta_o + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ... = \beta_o + \sum_{i=1}^N x_i \beta_i \ = XB + \beta_o\]
    \emph{$y$}: response/dependent variable being modeled (when multivariate, often a column vector by convention)
    \emph{$X$}: ``design matrix'' matrix of observations. Conventionally, each row is a realization and each column is a feature/variable
    \emph{$B$}: Coefficient or parameter vector
    \emph{$\beta_o$}: Constant term.

@Nonlinear features OK
    Features $(x_1,x_2,...)$ can be any function of recorded data. E.g. the following model is common for phase/direction tuning (used in eqn. 10 in Truccolo et al. 2005)
\[y = \beta_o + A\cdot cos(\varphi-\varphi_o)\]
\vspace{-2em}
    <2->\emph{$A$}: amplitude parameter
    <3->\emph{$\varphi$}: the observed phase or direction
    <4->\emph{$\varphi_o$}: preferred phase parameter
    <5->Can be written in a form that is linear in parameters
{overprint}
    \onslide<5|handout:0>
    \[y = \beta_o + A cos(\varphi_o) cos(\varphi) +  A sin(\varphi_o) sin(\varphi)\]
    \onslide<6>
    \[y = \beta_o + \beta_1 cos(\varphi) + \beta_2 sin(\varphi)\]

#############################################################################
$GLM point process for spike trains
$$ 
@Point process model:
    Truccolo et al. 2005:
        <1->``neural spike trains form a sequence of discrete events or point process time series''
        <2->``standard linear or nonlinear regression methods are designed for analysis of continuous-valued data and not point process observations''
    <3->Smoothing or binning can alter the structure
    <4->GLM Point-process models directly model spike trains without these drawbacks

@GLM point process models
    <1->Consider a homogeneous Poisson process with rate $\lambda$. 
        expected \# observations in time window $\Delta$ is $\lambda\Delta$
    <2->For inhomogeneous Poisson process rate varies time $\lambda(t)$
        \# observations from $t$ to $t+\Delta$ is $\int_t^{t+\Delta} \lambda(t) dt$
    <3->$\lambda(t)$ is called the "intensity function"
    <4->In a point process model we predict \emph{conditional intensity} based on measured covariates $\lambda(t|X(t))$
    <5->The Poisson GLM point process framework models conditional intensity functions of the form
{overprint}
    \onslide<5>
    \[\lambda(t|X(t)) = \exp \left\{ \mu + X B \right\}\]

#############################################################################

@Fitting GLM point-process models   
    <1->In practice spiking time series are discretized at some resolution $\Delta$
        <2->Predict sequence of spike counts $y_t=(y_1,y_2,...)$
        <3->With a discrete approximation of the conditional intensity $\lambda_t=(\lambda_1,\lambda_2,...)$
        <4->Based on discretely sampled covariates $X_t=(x_1,x_2,...)$
    <5->Typically $\Delta$ chosen such that $y_t\in\{0,1\}$, (larger bins can be used, making it a count process)
    <6->Model estimated by finding parameters $B$ that maximize the likelihood of the observed spikes $y$ and covariates $X$
        <7->Can be fit with iteratively reweighted least squares 
        <8->MATLAB glmfit \\ \href{http://www.mathworks.com/help/stats/glmfit.html}{\beamergotobutton{mathworks.com/help/stats/glmfit.htmlt}}
        <9->Python scikits.statsmodels.GLM \href{http://statsmodels.sourceforge.net/stable/glm.html}{\beamergotobutton{statsmodels.sourceforge.net/stable/glm.html}} 
        <10->I typically use gradient descent

@GLM point process model for single unit spiking with ensemble history
{overprint}
    \onslide<1|handout:0>
    \[\begin{aligned}\log[\lambda(t|X(t))\Delta] &= \mu \\
    &\phantom{{}+ \text{\emph{intrinsic history}}} \\
    &\phantom{{}+ \text{\emph{ensemble history}}}\\
    &\phantom{{}+ \text{\emph{extrinsic covariates}}}\end{aligned}\]
    \onslide<2|handout:0>
    \[\begin{aligned}\log[\lambda(t|X(t))\Delta] &= \mu \\
    &{}+ \text{\emph{intrinsic history}} \\
    &\phantom{{}+ \text{\emph{ensemble history}}}\\
    &\phantom{{}+ \text{\emph{extrinsic covariates}}}\end{aligned}\]
    \onslide<3|handout:0>
    \[\begin{aligned}\log[\lambda(t|X(t))\Delta] &= \mu \\
    &{}+ \text{\emph{intrinsic history}} \\
    &{}+ \text{\emph{ensemble history}}\\
    &\phantom{{}+ \text{\emph{extrinsic covariates}}}\end{aligned}\]
    \onslide<4>
    \[\begin{aligned}\log[\lambda(t|X(t))\Delta] &= \mu \\
    &{}+ \text{\emph{intrinsic history}} \\
    &{}+ \text{\emph{ensemble history}}\\
    &{}+ \text{\emph{extrinsic covariates}}\end{aligned}\]

@Intrinsic history filter
    <1->Theoretically, model the whole history \[\lambda_t(t|\theta,y_{t-1},y_{t-2},...,y_1) \propto \exp\left\{ \sum_{\tau=1}^t \gamma_\tau y_{t-\tau} \right\}\]
    <2->In practice: use finite history duration with \emph{basis functions} (a form of regularization: enforce smoothness and reduce parameters) \[\lambda_t(t|\theta,y_{t-\tau:t-1}) \propto \exp\left\{ B \cdot y_{t-\tau:t-1} \right\}\]
{overprint}
    \onslide<2>
    [intrinsic_basis,0.3]

@Ensemble history filter
    <1->Ensemble history filters are treated similarly to intrinsic history
    <2->Typically use fewer basis functions than for the intrinsic history, otherwise the number of parameters becomes prohibitive
    <3->If using regularization, typically each neuron's parameters are penalized as a group (sparse connectivity prior)
    <4->More on intrinsic history and network connectivity next week

@Extrinsic covariates: kinematics
    <1->Truccolo et al. 2005 explore a 2D velocity tuning model based on the $x$ and $y$ components of hand velocity. 
    <2->Hatsopoulos et al. 2007 use normalized, extended velocity trajectories "pathlets"
#    <3->Rule et al. 2015 found Hatsopoulos normalized "pathlets" and position trajectories to perform equivalently


#############################################################################

#############################################################################

@Maximum likelihood approach to model fitting
    <1->Let $y_t$ be an inhomogeneous Poisson with time varying rate $\lambda_t$
    <2->The probability of observing $k$ spikes in a time interval $\Delta$ is Poisson distributed\[\Pr(y_t=k) \approx (\Delta \lambda_t)^{y_t} \frac{ e^{-\Delta \lambda_t}}{y_t!}\]
    <3->Assuming conditional independence, the probability of observing an entire sequence $y_t$ is \[\Pr(y|\lambda) = \prod_{t=1}^T (\Delta\lambda_t)^{y_t} e^{-\Delta\lambda_t} / {y_t}! \]
    
@Minimize the negative log-likelihood
    <1->Fit the model by finding the parameters $\mu$, $B$ that \emph{maximize the likelihood} $\mathcal{L}(\mu,B|y) = \Pr(y|\mu,B)$ of the observations\footnote{Note: writing $\lambda_t$ here instead of $\Delta\lambda_t$, i.e. let $\Delta=1$. In this case, parameters $\mu$ and $B$ will take units of $\Delta$} \[\Pr(y|\mu,B) = \prod_{t=1}^T \lambda_t^{y_t} e^{-\lambda_t} / {y_t}! \]\[\lambda_t=\exp(\mu + X_t B)\]
    <2->In practice, \emph{minimize the negative log-likelihood}, which, if $\Delta$ is small $s.t.$ $y_t$ is always 0 or 1 \[ -\ln\mathcal{L}(\mu,B|y) =  \sum_{t=1}^T[ \lambda_t - y_t \ln(\lambda_t) ]\]
    
@Gradient of the negative log-likelihood
    <1->Let $f(\mu,B)$ be the negative log likelihood \[f(\mu,B) = -\ln\mathcal{L}(\mu,B|y) =  \sum_{t=1}^T[ \lambda_t - y_t \ln(\lambda_t) ]\]
    <2->Substitute our model $\lambda_t=e^{\mu + X_t B}$ \[f(\mu,B) =  \sum_{t=1}^T[ e^{\mu + X_t B} - y_t (\mu + X_t B) ]\]
    <3->The partial derivatives, w.r.t $\mu$ and $(\beta_1,\beta_2,...)=B$ are:\[\begin{aligned}\frac{\partial f}{\partial    \mu}=&\sum_{t=1}^T[e^{\mu+X_t B}-y_t]\\\frac{\partial f}{\partial\beta_i}=&\sum_{t=1}^T[X_{t,i} e^{\mu+X_t B}-y_t X_{t,i}]\end{aligned}\]
    <4->Hessian is straightforward (can speed up optimization)

#############################################################################
$$Regularization

@Regularized GLM 
    <1->Cross-validation is necessary to assess over-fitting: 
        data should be separated into training and testing sets
    <2->For larger number of parameters, the model \emph{will} overfit:
        regularization is necessary. 
    <3->Regularization can be incorporated by adding a penalty term to the negative log-likelihood function \[\argmin_{\mu,B}\left\{ \textrm{\emph{Penalty}}(B) - \ln\mathcal{L}(\mu,B|y) \right\}\]
    <4->Conjugate gradient solvers are useful here

@Regularized GLM: L1 and L2
    <1->L2 penalty: Parameters penalized by their squared magnitudes\[  \alpha \sum_{i=1}^N \beta_i^2 \]
        <2->Equivalent to a Gaussian prior on parameters
        <3->Can be solved with gradient descent
    <4->L1 penalty: Parameters penalized by their absolute magnitude\[ \alpha \sum_{i=1}^N |\beta_i| \]
        <5->Promotes $\beta_i=0$, useful for finding sparse solutions
        <6->Discontinuous gradient at $\beta_i=0$ precludes gradient descent. 
        <7->Can use coordinate descent (although we ran into convergence issues?)

@Regularized GLM: L1 approximation and L0
    <1->~$\sqrt{x^2+\epsilon}:$ Smooth approximation of L1 penalty that is suitable for gradient descent\footnote{Called "Charbonnier penalty" in the computer vision literature (Charbonnier et al. 1994)}\[ \alpha \sum_{i=1}^N \sqrt{\beta_i^2+\varepsilon} \]
        <2->$\epsilon$ is chosen to be small, but strictly positive
    <3->L0 penalty: Constant penalty if a parameter is nonzero\[ \alpha \sum_{i=1}^N \delta(\beta_i\ne 0) \]
        <4->Computationally infeasible
        <5->Greedy algorithms are a good (the best polynomial time?) approximation

@Regularized GLM: Group lasso
    <1->Concept: penalize \emph{groups} of parameters with the L1 norm\[ \alpha \sum_{i=1}^N \sqrt{ \sum_{j=1}^K \beta_{i,j}^2 } \]
        <2->Useful for penalizing ensemble filters as a group for each neuron
        <3->Derivative is undefined at $\sqrt{ \sum_{j=1}^K \beta_{i,j}^2 }=0$
        <4->Roth \& Fischer 2008\footnote{Roth, Volker, and Bernd Fischer. "The group-lasso for generalized linear models: uniqueness of solutions and efficient algorithms." Proceedings of the 25th international conference on Machine learning. ACM, 2008.} discuss an efficient fitting procedure
    <5->Sparse group lasso
        <6->Simon, Noah, et al. "A sparse-group lasso." Journal of Computational and Graphical Statistics 22.2 (2013): 231-245.
        <7->Note: we were unable to get the Matlab version of this library to function on our data. It appears to be a known bug for large datasets.\href{goo.gl/kFXMqU}{\beamergotobutton{goo.gl/kFXMqU}}

#############################################################################

@Two-layer crossvalidation
    <1->The strength of regularization $\alpha$ is another free parameter
        Can be fixed in advance or...
        <2->Two-level crossvalidation can be used to estimate $\alpha$ from the data
    <3->K-fold two-layer crossvalidation results in K${}^2$ parameter fitting steps
        This can get slow on large datasets
        
        
#############################################################################

@Regularization paths
    Regularization paths:
        When evaluating a range of regularization parameters e.g. $\alpha=(0,0.1,1,20)$,
        Carry over the model weights $\mu$ and $\beta$. 
        That is, for example, start the parameter tuning for $\alpha=1$ with the $\mu,B$ returned from $\alpha=0.1$

@Initialization with closed form solution
    <1->Fitting many models using a large amount of data can take a very long time
    <2->Using an approximation to estimate a good starting location can give some speedup
    <2->Ramirez, A. D., \& Paninski, L. (2014). Fast inference in generalized linear models via expected log-likelihoods. Journal of computational neuroscience, 36(2), 215-234.

#############################################################################
@Discussion points from neurostats meeting 2015/09/21
What does GLM point process buy you over say spike-triggered averaging or correlation analyses?
    No assumptions of binning or smoothing timescale required
    Can combine multiple feature sets with very different properties in a single model
        E.g. could combine position information, phase tuning, and spiking history in a single model
    In my experience, GLMs point process models display the best performance in terms of predicting spiking.
    
#############################################################################
@Discussion points from neurostats meeting 2015/09/21
What other alternatives are there to the GLM point process?
    Any model that predicts spiking probability in a small time bin
        But support vector machines don't seem to work that well. 
    Binary classifiers, or use Bayes' theorem to estimate from observations \[\Pr(y_t=1 | X_t) = \frac{\Pr(X_t|y_t=1)}{\Pr(X_t)}{\Pr(y_t=1)}\]
    $\Pr(X|y)$ and $\Pr(X)$ can be modeled with the usual density estimation approaches
        e.g. Assuming multivariate gaussian distributions, or other multivariate distributions
        Nonparametric density estimators, Naïve Bayes
        Works OK if modeling assumptions are correct. In my experience GLM point process is more robust. 
        
@Discussion points from neurostats meeting 2015/09/21
Using distributions from the exponential family in a Bayesian model results in models that are log-linear in the parameters and have analogous GLM point-process models.
~
\[\Pr(y_t=1 | X_t) \propto \frac
{\Pr(X_t|y_t=1)}
{\Pr(X_t)}
\]

The exponential family canonical form for feature vector $\mathbf{x}$ and parameter vector ${\boldsymbol \theta}$:
\[
\Pr(\mathbf{x}|\boldsymbol \theta) = h(\mathbf{x}) g(\boldsymbol \theta) \exp\left\{{\boldsymbol \theta} \cdot \mathbf{T}(\mathbf{x})\right\}
\]

@
END

@Discussion points from neurostats meeting 2015/09/21
The general form of a Bayesian spiking model using distributions of the exponential family 
\[\Pr(y_t=1 | X_t) \propto \frac {
h_1(X_t) g_1(\boldsymbol \theta_1) \exp \left\{{\boldsymbol \theta_1} \cdot \mathbf{T_1}(X_t) \right\}}{
h_0(X_t) g_0(\boldsymbol \theta_0) \exp \left\{{\boldsymbol \theta_0} \cdot \mathbf{T_0}(X_t) \right\}}
\]
In log form
\[\textstyle 
\begin{aligned}
\ln\Pr(y_t=1 | X_t) = \mu
&+ \ln h_1(X_t)
+ \ln g_1(\boldsymbol \theta_1)
+ {\boldsymbol \theta_1} \cdot \mathbf{T_1}(X_t)\\
~&- \ln h_0(X_t) 
- \ln g_0(\boldsymbol \theta_0) 
- {\boldsymbol \theta_0} \cdot \mathbf{T_0}(X_t)
\end{aligned}
\]


@Discussion points from neurostats meeting 2015/09/21
The normalization factors can be absorbed into the constant term
\[\textstyle 
\begin{aligned}
\ln\Pr(y_t=1 | X_t) = \mu
&+ \ln h_1(X_t)
+ {\boldsymbol \theta_1} \cdot \mathbf{T_1}(X_t)\\
~&- \ln h_0(X_t) 
- {\boldsymbol \theta_0} \cdot \mathbf{T_0}(X_t)
\end{aligned}
\]
This model is log-linear in parameters. If distributions for marginal $X$ and spike-triggered $X$ use the same $h$ and $\mathbf{T}$, the form is simpler:
\[
\ln\Pr(y_t=1 | X_t) = \mu + ({\boldsymbol \theta_1}-{\boldsymbol \theta_0}) \cdot \mathbf{T}(X_t) = \mu + \mathbf{B}\cdot\mathbf{T}(X_t)
\]
Directly optimizing this GLM gives better results than estimating the parameters from the spike-triggered and marginal distributions of $X$ separately. (in my experience)







